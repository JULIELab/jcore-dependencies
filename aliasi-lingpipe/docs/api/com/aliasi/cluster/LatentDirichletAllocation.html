<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_14) on Thu Jun 23 19:27:25 EDT 2011 -->
<TITLE>
LatentDirichletAllocation (LingPipe API)
</TITLE>

<META NAME="date" CONTENT="2011-06-23">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="LatentDirichletAllocation (LingPipe API)";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../com/aliasi/cluster/KMeansClusterer.html" title="class in com.aliasi.cluster"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../index.html?com/aliasi/cluster/LatentDirichletAllocation.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="LatentDirichletAllocation.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;<A HREF="#nested_class_summary">NESTED</A>&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
com.aliasi.cluster</FONT>
<BR>
Class LatentDirichletAllocation</H2>
<PRE>
<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">java.lang.Object</A>
  <IMG SRC="../../../resources/inherit.gif" ALT="extended by "><B>com.aliasi.cluster.LatentDirichletAllocation</B>
</PRE>
<DL>
<DT><B>All Implemented Interfaces:</B> <DD><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/Serializable.html?is-external=true" title="class or interface in java.io">Serializable</A></DD>
</DL>
<HR>
<DL>
<DT><PRE>public class <B>LatentDirichletAllocation</B><DT>extends <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</A><DT>implements <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/Serializable.html?is-external=true" title="class or interface in java.io">Serializable</A></DL>
</PRE>

<P>
A <code>LatentDirichletAllocation</code> object represents a latent
 Dirichlet allocation (LDA) model.  LDA provides a Bayesian model of
 document generation in which each document is generated by
 a mixture of topical multinomials.  An LDA model specifies the
 number of topics, a Dirichlet prior over topic mixtures for a
 document, and a discrete distribution over words for each topic.

 <p>A document is generated from an LDA model by first selecting a
 multinomial over topics given the Dirichlet prior.  Then for each
 token in the document, a topic is generated from the
 document-specific topic distribution, and then a word is generated
 from the discrete distribution for that topic.  Note that document
 length is not generated by this model; a fully generative model
 would need a way of generating lengths (e.g. a Poisson
 distribution) or terminating documents (e.g. a disginuished
 end-of-document symbol).

 <p>An LDA model may be estimated from an unlabeled training corpus
 (collection of documents) using a second Dirichlet prior, this time
 over word distributions in a topic.  This class provides a static
 inference method that produces (collapsed Gibbs) samples from the
 posterior distribution of topic assignments to words, any one of
 which may be used to construct an LDA model instance.

 <p>An LDA model can be used to infer the topic mixture of unseen
 text documents, which can be used to compare documents by topical
 similarity.  A fixed LDA model may also be used to estimate the
 likelihood of a word occurring in a document given the other words
 in the document.  A collection of LDA models may be used for fully
 Bayesian reasoning at the corpus (collection of documents) level.

 <p>LDA may be applied to arbitrary multinomial data.  To apply it
 to text, a tokenizer factory converts a text document to bag of
 words and a symbol table converts these tokens to integer outcomes.
 The static method <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#tokenizeDocuments(java.lang.CharSequence[], com.aliasi.tokenizer.TokenizerFactory, com.aliasi.symbol.SymbolTable, int)"><CODE>tokenizeDocuments(CharSequence[],TokenizerFactory,SymbolTable,int)</CODE></A>
 is available to carry out the text to multinomial conversion with
 pruning of low counts.


 <h4>LDA Generative Model</h4>

 <p>LDA operates over a fixed vocabulary of discrete outcomes, which
 we call words for convenience, and represent as a set of integers:

 <blockquote><pre>
 words = { 0, 1, ..., numWords-1 }</pre></blockquote>

 <p>A corpus is a ragged array of documents, each document consisting
 of an array of words:

 <blockquote><pre>
 int[][] words = { words[0], words[1], ..., words[numDocs-1] }</pre></blockquote>

 A given document <code>words[i]</code>, <code>i &lt; numDocs</code>,
 is itself represented as a bag of words, each word being
 represented as an integer:

 <blockquote><pre>
 int[] words[i] = { words[i][0], words[i][1], ..., words[i][words[i].length-1] }</pre></blockquote>

 The documents do not all need to be the same length, so the
 two-dimensional array <code>words</code> is ragged.

 <p>A particular LDA model is defined over a fixed number of topics,
 also represented as integers:

 <blockquote><pre>
 topics = { 0, 1, ..., numTopics-1 }</pre></blockquote>

 <p>For a given topic <code>topic &lt; numTopics</code>, LDA specifies a
 discrete distribution <code>&phi;[topic]</code> over words:

 <blockquote><pre>
 &phi;[topic][word] &gt;= 0.0

 <big><big><big>&Sigma;</big></big></big><sub><sub>word &lt; numWords</sub></sub> &phi;[topic][word] = 1.0</pre></blockquote>

 <p>In an LDA model, each document in the corpus is generated from a
 document-specific mixture of topics <code>&theta;[doc]</code>.  The
 distribution <code>&theta;[doc]</code> is a discrete distribution over
 topics:

 <blockquote><pre>
 &theta;[doc][topic] &gt;= 0.0

 <big><big><big>&Sigma;</big></big></big><sub><sub>topic &lt; numTopics</sub></sub> &theta;[doc][topic] = 1.0</pre></blockquote>

 <p>Under LDA's generative model for documents, a document-specific
 topic mixture <code>&theta;[doc]</code> is generated from a uniform
 Dirichlet distribution with concentration parameter
 <code>&alpha;</code>.  The Dirichlet is the conjugate prior for the
 multinomial in which <code>&alpha;</code> acts as a prior count
 assigned to a topic in a document.  Typically, LDA is run with a
 fairly diffuse prior with concentration <code>&alpha; &lt;
 1</code>, leading to skewed posterior topic distributions.

 <p>Given a topic distribution <code>&theta;[doc]</code> for a
 document, tokens are generated (conditionally) independently.  For
 each token in the document, a topic
 <code>topics[doc][token]</code> is generated according to the
 topic distribution <code>&theta;[doc]</code>, then the word instance
 <code>words[doc][token]</code> is
 generated given the topic using the topic-specific distribution
 over tokens <code>&phi;[topics[doc][token]]</code>.

 <p>For estimation purposes, LDA places a uniform Dirichlet prior
 with concentration parameter <code>&beta;</code> on each of the
 topic distributions <code>&phi;[topic]</code>.  The first step in
 modeling a corpus is to generate the topic distributions
 <code>&phi;</code> from a Dirichlet parameterized by
 <code>&beta;</code>.

 <p>In sampling notation, the LDA generative model is expressed as
 follows:

 <blockquote><pre>
 &phi;[topic]           ~ Dirichlet(&beta;)
 &theta;[doc]             ~ Dirichlet(&alpha;)
 topics[doc][token] ~ Discrete(&theta;[doc])
 words[doc][token]  ~ Discrete(&phi;[topic[doc][token]])</pre></blockquote>

 <p>A generative Bayesian model is based on a the joint probablity
 of all observed and unobserved variables, given the priors.  Given a
 text corpus, only the words are observed.  The unobserved variables
 include the assignment of a topic to each word, the assignment of a
 topic distribution to each document, and the assignment of word
 distributions to each topic.  We let
 <code>topic[doc][token]</code>, <code>doc &lt; numDocs, token &lt;
 docLength[doc]</code>, be the topic assigned to the specified token
 in the specified document.  This leaves two Dirichlet priors, one
 parameterized by <code>&alpha;</code> for topics in documents, and
 one parameterized by <code>&beta;</code> for words in topics.
 These priors are treated as hyperparameters for purposes of
 estimation; cross-validation may be used to provide so-called empirical
 Bayes estimates for the priors.

 <p>The full LDA probability model for a corpus follows the generative
 process outlined above.  First, topic distributions are chosen at the
 corpus level for each topic given their Dirichlet prior, and then the
 remaining variables are generating given these topic distributions:

 <blockquote><pre>
 p(words, topics, &theta;, &phi | &alpha;, &beta)
 = <big><big><big>&Pi;</big></big></big><sub><sub>topic &lt; numTopics</sub></sub>
        p(&phi;[topic] | &beta;)
        * p(words, topics, &theta; | &alpha;, &phi;)</pre></blockquote>

 Note that because the multinomial parameters for
 <code>&phi;[topic]</code> are continuous,
 <code>p(&phi;[topic] | &beta;)</code> represents a density, not a discrete
 distribution.  Thus it does not make sense to talk about the
 probability of a given multinomial <code>&phi;[topic]</code>; non-zero
 results only arise from integrals over measurable subsets of the
 multinomial simplex.  It is possible to sample from a density, so
 the generative model is well founded.

 <p>A document is generated by first generating its topic distribution
 given the Dirichlet prior, and then generating its topics and words:

 <blockquote><pre>
 p(words, topics, &theta; | &alpha;, &phi;)
 = <big><big><big>&Pi;</big></big></big><sub><sub>doc &lt; numDocs</sub></sub>
        p(&theta;[doc] | &alpha;)
        * p(words[doc], topics[doc] | &theta;[doc], &phi;)</pre></blockquote>

 The topic and word are generated from the multinomials
 <code>&theta;[doc]</code> and the topic distributions
 <code>&phi;</code> using the chain rule, first generating the topic
 given the document's topic distribution and then generating the
 word given the topic's word distribution.

 <blockquote><pre>
 p(words[doc], topics[doc] | &theta;[doc], &phi;)
 = <big><big><big>&Pi;</big></big></big><sub><sub>token &lt; words[doc].length</sub></sub>
        p(topics[doc][token] | &theta;[doc])
        * p(words[doc][token] | &phi;[topics[doc][token]])</pre></blockquote>

 <p>Given the topic and document multinomials, this distribution
 is discrete, and thus may be evaluated.  It may also be marginalized
 by summation:

 <blockquote><pre>
 p(words[doc] | &theta;[doc], &phi;)
 = <big><big><big>&Pi;</big></big></big><sub><sub> token &lt; words[doc].length</sub></sub>
       <big><big><big>&Sigma;</big></big></big><sub><sub>topic &lt; numTopics</sub></sub> p(topic | &theta;[doc]) * p(words[doc][token] | &phi;[topic])</pre></blockquote>

 <p>Conditional probablities are computed in the usual way by
 marginalizing other variables through integration.  Unfortunately,
 this simple mathematical operation is often intractable
 computationally.


 <h3>Estimating LDA with Collapsed Gibbs Sampling</h3>

 <p>This class uses a collapsed form of Gibbs sampling over the
 posterior distribution of topic assignments given the documents
 and Dirichlet priors:

 <blockquote><pre>
 p(topics | words, &alpha; &beta;)</pre></blockquote>

 <p>This distribution may be derived from the joint
 distribution by marginalizing (also known as &quot;collapsing&quot;
 or &quot;integrating out&quot;) the contributions of the
 document-topic and topic-word distributions.

 <p>The Gibbs sampler used to estimate LDA models produces samples
 that consist of a topic assignment to every token in the corpus.
 The conjugacy of the Dirichlet prior for multinomials makes the
 sampling straightforward.

 <p>An initial sample is produced by randomly assigning topics to
 tokens.  Then, the sampler works iteratively through the corpus,
 one token at a time.  At each token, it samples a new topic assignment
 to that token given all the topic assignments to other tokens in the
 corpus:

 <blockquote><pre>
 p(topics[doc][token] | words, topics')</pre></blockquote>

 The notation <code>topics'</code> represents the set of topic
 assignments other than to <code>topics[doc][token]</code>.  This
 collapsed posterior conditional is estimated directly:

 <blockquote><pre>
 p(topics[doc][token] = topic | words, topics')
 = (count'(doc,topic) + &alpha;) / (docLength[doc]-1 + numTopics*&alpha;)
 * (count'(topic,word) + &beta;) / (count'(topic) + numWords*&beta;)</pre></blockquote>

 The counts are all defined relative to <code>topics'</code>; that
 is, the current topic assignment for the token being sampled is not
 considered in the counts.  Note that the two factors are estimates
 of <code>&theta;[doc]</code> and <code>&phi;[topic]</code> with all
 data other than the assignment to the current token.  Note how the
 prior concentrations arise as additive smoothing factors in these
 estimates, a result of the Dirichlet's conjugacy to the
 multinomial.  For the purposes of sampling, the document-length
 normalization in the denominator of the first term is not necessary,
 as it remains constant across topics.

 <p>The posterior Dirichlet distributions may be computed using just
 the counts.  For instance, the posterior distribution for topics in
 documents is estimated as:

 <blockquote><pre>
 p(&theta[doc]|&alpha;, &beta;, words)
 = Dirichlet(count(doc,0)+&beta;, count(doc,1)+&beta;, ..., count(doc,numTopics-1)+&beta;)</pre></blockquote>

 <p>The sampling distribution is defined from the maximum a posteriori
 (MAP) estimates of the multinomial distribution over topics in a document:

 <blockquote><pre>
 &theta;<sup>*</sup>[doc] = ARGMAX<sub><sub>&theta;[doc]</sub></sub> p(&theta;[doc] | &alpha;, &beta;, words)</pre></blockquote>

 which we know from the Dirichlet distribution is:

 <blockquote><pre>
 &theta;<sup>*</sup>[doc][topic]
 = (count(doc,topic) + &alpha;) / (docLength[doc] + numTopics*&alpha;)</pre></blockquote>

 By the same reasoning, the MAP word distribution in topics is:

 <blockquote><pre>
 &phi;<sup>*</sup>[topic][word]
 = (count(topic,word) + &beta;) / (count(topic) + numWords*&beta;)</pre></blockquote>

 <p>A complete Gibbs sample is represented as an instance of <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster"><CODE>LatentDirichletAllocation.GibbsSample</CODE></A>, which provides access to
 the topic assignment to every token, as well as methods to compute
 <code>&theta;<sup>*</sup></code> and <code>&phi;<sup>*</sup></code>
 as defined above.  A sample also maintains the original priors and
 word counts.  Just the estimates of the topic-word distributions
 <code>&phi;[topic]</code> and the prior topic concentration
 <code>&alpha;</code> are sufficient to define an LDA model.  Note
 that the imputed values of <code>&theta;<sup>*</sup>[doc]</code>
 used during estimation are part of a sample, but are not part of
 the LDA model itself.  The LDA model contains enough information to
 estimate <code>&theta;<sup>*</sup></code> for an arbitrary
 document, as described in the next section.

 <p>The Gibbs sampling algorithm starts with a random assignment of
 topics to words, then simply iterates through the tokens in turn,
 sampling topics according to the distribution defined above.  After
 each run through the entire corpus, a callback is made to a handler
 for the samples.  This setup may be configured for
 an initial burnin period, essentially just discarding the first
 batch of samples.  Then it may be configured to sample only periodically
 thereafter to avoid correlations between samples.

 <h3>LDA as Multi-Topic Classifier</h3>

 <p>An LDA model consists of a topic distribution Dirichlet prior
 <code>&alpha;</code> and a word distribution
 <code>&phi;[topic]</code> for each topic.  Given an LDA model and a
 new document <code>words = { words[0], ..., words[length-1] }</code>
 consisting of a sequence of words, the posterior distribution over
 topic weights is given by:

 <blockquote><pre>
 p(&theta; | words, &alpha;, &phi;)</pre></blockquote>

 Although this distribution is not solvable analytically, it is easy
 to estimate using a simplified form of the LDA estimator's Gibbs
 sampler.  The conditional distribution of a topic assignment
 <code>topics[token]</code> to a single token given an assignment
 <code>topics'</code> to all other tokens is given by:

 <blockquote><pre>
 p(topic[token] | topics', words, &alpha;, &phi;)
 &#8733; p(topic[token], words[token] | topics', &alpha; &phi;)
 = p(topic[token] | topics', &alpha;) * p(words[token] | &phi;[topic[token]])
 = (count(topic[token]) + &alpha;) / (words.length - 1 + numTopics * &alpha;)
   * p(words[token] | &phi;[topic[token]])</pre></blockquote>

 This leads to a straightforward sampler over posterior topic
 assignments, from which we may directly compute the Dirichlet posterior over
 topic distributions or a MAP topic distribution.

 <p>This class provides a method to sample these topic assignments,
 which may then be used to form Dirichlet distributions or MAP point
 estimates of <code>&theta;<sup>*</sup></code> for the document
 <code>words</code>.

 <h3>LDA as a Conditional Language Model</h3>

 <p>An LDA model may be used to estimate the likelihood of a word
 given a previous bag of words:

 <blockquote><pre>
 p(word | words, &alpha;, &phi;)
 = <big><big><big><big>&#8747;</big></big></big></big>p(word | &theta;, &phi;) p(&theta; | words, &alpha;, &phi;) <i>d</i>&theta;</pre></blockquote>

 This integral is easily evaluated using sampling over the topic
 distributions <code>p(&theta; | words, &alpha;, &phi;)</code> and
 averaging the word probability determined by each sample.
 The word probability for a sample <code>&theta;</code> is defined by:

 <blockquote><pre>
 p(word | &theta;, &phi;)
 = <big><big><big>&Sigma;</big></big></big><sub><sub>topic &lt; numTopics</sub></sub> p(topic | &theta;) * p(word | &phi;[topic])</pre></blockquote>

 Although this approach could theoretically be applied to generate
 the probability of a document one word at a time, the cost would
 be prohibitive, as there are quadratically many samples required
 because samples for the <code>n</code>-th word consist of topic
 assignments to the previous <code>n-1</code> words.

 <!--
 <h3>LDA as a Language Model</h3>

 The likelihood of a document relative to an LDA model is defined
 by integrating over all possible topic distributions weighted
 by prior likelihood:

 <blockquote><pre>
 p(words | &alpha;, &phi;)
 = <big><big><big>&#8747;</big></big></big> p(words | &theta;, &phi;) * p(&theta; | &alpha;) <i>d</i>&theta;</pre></blockquote>

 The probability of a document is just the product of
 the probability of its tokens:

 <blockquote><pre>
 p(words | &theta;, &phi;)
 = <big><big><big>&Pi;</big></big></big><sub><sub>token &lt; words.length</sub></sub> p(words[token] | &theta;, &phi;)</pre></blockquote>

 The probability of a word given a topic distribution and a
 per-topic word distribution is derived by summing its probabilities
 over all topics, weighted by topic probability:

 <blockquote><pre>
 p(word | &theta;, &phi;)
 = <big><big><big>&Sigma;</big></big></big><sub><sub>topic &lt; numTopics</sub></sub> p(topic | &theta;) * p(word | &phi;[topic])</pre></blockquote>

 Unfortunately, this value is not easily computed using Gibbs
 sampling.  Although various estimates exist in the literature,
 they are quite expensive to compute.
 -->

 <h3>Bayesian Calculations and Exchangeability</h3>

 <p>An LDA model may be used for a variety of statistical
 calculations.  For instance, it may be used to determine the
 distribution of topics to words, and using these distributions, may
 determine word similarity.  Similarly, document similarity may be
 determined by the topic distributions in a document.

 <p>Point estimates are derived using a single LDA model.  For
 Bayesian calculation, multiple samples are taken to produce
 multiple LDA models.  The results of a calculation on these
 models is then averaged to produce a Bayesian estimate of the
 quantity of interest.  The sampling methodology is effectively
 numerically computing the integral over the posterior.

 <p>Bayesian calculations over multiple samples are complicated by
 the exchangeability of topics in the LDA model.  In particular, there
 is no guarantee that topics are the same between samples, thus it
 is not acceptable to combine samples in topic-level reasoning.  For
 instance, it does not make sense to estimate the probability of
 a topic in a document using multiple samples.


 <h3>Non-Document Data</h3>

 The &quot;words&quot; in an LDA model don't necessarily have to
 represent words in documents.  LDA is basically a multinomial
 mixture model, and any multinomial outcomes may be modeled with
 LDA.  For instance, a document may correspond to a baseball game
 and the words may correspond to the outcomes of at-bats (some
 might occur more than once).  LDA has also been used for
 gene expression data, where expression levels from mRNA microarray
 experiments is quantized into a multinomial outcome.

 <p>LDA has also been applied to collaborative filtering.  Movies
 act as words, with users modeled as documents, the bag of words
 they've seen.  Given an LDA model and a user's films, the user's
 topic distribution may be inferred and used to estimate the likelihood
 of seeing unseen films.

 <h3>References</h3>

 <ul>
 <li>Wikipedia: <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a>
 <li>Wikipedia: <a href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a>
 <li>Wikipedia: <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet Distribution</a>
 <li>Wikipedia: <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_Allocation">Latent Dirichelt Distribution</a>
 <li>Steyvers, Mark and Tom Griffiths. 2007.
     <a href="http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf">Probabilistic topic models</a>.
     In  Thomas K. Landauer, Danielle S. McNamara, Simon Dennis and Walter Kintsch (eds.),
     <i>Handbook of Latent Semantic Analysis.</i>
     Lawrence Erlbaum.</li>
       <!-- alt link: http://cocosci.berkeley.edu/tom/papers/SteyversGriffiths.pdf -->

 <li>Blei, David M., Andrew Y. Ng, and Michael I. Jordan.  2003.
       <a href="http://jmlr.csail.mit.edu/papers/v3/blei03a.html">Latent Dirichlet allocation</a>.
       <i>Journal of Machine Learning Research</i> <b>3</b>:993-1022.</li>
       <!-- http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf -->
 </ul>
<P>

<P>
<DL>
<DT><B>Since:</B></DT>
  <DD>LingPipe3.3</DD>
<DT><B>Version:</B></DT>
  <DD>4.0.0</DD>
<DT><B>Author:</B></DT>
  <DD>Bob Carpenter</DD>
<DT><B>See Also:</B><DD><A HREF="../../../serialized-form.html#com.aliasi.cluster.LatentDirichletAllocation">Serialized Form</A></DL>
<HR>

<P>
<!-- ======== NESTED CLASS SUMMARY ======== -->

<A NAME="nested_class_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Nested Class Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;class</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A></B></CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The <code>LatentDirichletAllocation.GibbsSample</code> class
 encapsulates all of the information related to a single Gibbs
 sample for latent Dirichlet allocation (LDA).</TD>
</TR>
</TABLE>
&nbsp;
<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#LatentDirichletAllocation(double, double[][])">LatentDirichletAllocation</A></B>(double&nbsp;docTopicPrior,
                          double[][]&nbsp;topicWordProbs)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Construct a latent Dirichelt allocation (LDA) model using the
 specified document-topic prior and topic-word distributions.</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#bayesTopicEstimate(int[], int, int, int, java.util.Random)">bayesTopicEstimate</A></B>(int[]&nbsp;tokens,
                   int&nbsp;numSamples,
                   int&nbsp;burnin,
                   int&nbsp;sampleLag,
                   <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the Bayesian point estimate of the topic distribution
 for a document consisting of the specified tokens, using Gibbs
 sampling with the specified parameters.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#documentTopicPrior()">documentTopicPrior</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the concentration value of the uniform Dirichlet prior over
 topic distributions for documents.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Iterator.html?is-external=true" title="class or interface in java.util">Iterator</A>&lt;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A>&gt;</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#gibbsSample(int[][], short, double, double, java.util.Random)">gibbsSample</A></B>(int[][]&nbsp;docWords,
            short&nbsp;numTopics,
            double&nbsp;docTopicPrior,
            double&nbsp;topicWordPrior,
            <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return an iterator over Gibbs samples for the specified
 document-word corpus, number of topics, priors and randomizer.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#gibbsSampler(int[][], short, double, double, int, int, int, java.util.Random, com.aliasi.corpus.ObjectHandler)">gibbsSampler</A></B>(int[][]&nbsp;docWords,
             short&nbsp;numTopics,
             double&nbsp;docTopicPrior,
             double&nbsp;topicWordPrior,
             int&nbsp;burninEpochs,
             int&nbsp;sampleLag,
             int&nbsp;numSamples,
             <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random,
             <A HREF="../../../com/aliasi/corpus/ObjectHandler.html" title="interface in com.aliasi.corpus">ObjectHandler</A>&lt;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A>&gt;&nbsp;handler)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Run Gibbs sampling for the specified multinomial data, number
 of topics, priors, search parameters, randomization and
 callback sample handler.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#numTopics()">numTopics</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of topics in this LDA model.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#numWords()">numWords</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of words on which this LDA model
 is based.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;short[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#sampleTopics(int[], int, int, int, java.util.Random)">sampleTopics</A></B>(int[]&nbsp;tokens,
             int&nbsp;numSamples,
             int&nbsp;burnin,
             int&nbsp;sampleLag,
             <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the specified number of Gibbs samples of topics for the
 specified tokens using the specified number of burnin epochs,
 the specified lag between samples, and the specified
 randomizer.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;int[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#tokenizeDocument(java.lang.CharSequence, com.aliasi.tokenizer.TokenizerFactory, com.aliasi.symbol.SymbolTable)">tokenizeDocument</A></B>(<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/CharSequence.html?is-external=true" title="class or interface in java.lang">CharSequence</A>&nbsp;text,
                 <A HREF="../../../com/aliasi/tokenizer/TokenizerFactory.html" title="interface in com.aliasi.tokenizer">TokenizerFactory</A>&nbsp;tokenizerFactory,
                 <A HREF="../../../com/aliasi/symbol/SymbolTable.html" title="interface in com.aliasi.symbol">SymbolTable</A>&nbsp;symbolTable)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tokenizes the specified text document using the specified tokenizer
 factory returning only tokens that exist in the symbol table.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;int[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#tokenizeDocuments(java.lang.CharSequence[], com.aliasi.tokenizer.TokenizerFactory, com.aliasi.symbol.SymbolTable, int)">tokenizeDocuments</A></B>(<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/CharSequence.html?is-external=true" title="class or interface in java.lang">CharSequence</A>[]&nbsp;texts,
                  <A HREF="../../../com/aliasi/tokenizer/TokenizerFactory.html" title="interface in com.aliasi.tokenizer">TokenizerFactory</A>&nbsp;tokenizerFactory,
                  <A HREF="../../../com/aliasi/symbol/SymbolTable.html" title="interface in com.aliasi.symbol">SymbolTable</A>&nbsp;symbolTable,
                  int&nbsp;minCount)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tokenize an array of text documents represented as character
 sequences into a form usable by LDA, using the specified
 tokenizer factory and symbol table.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#wordProbabilities(int)">wordProbabilities</A></B>(int&nbsp;topic)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns an array representing of probabilities of words in the
 specified topic.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#wordProbability(int, int)">wordProbability</A></B>(int&nbsp;topic,
                int&nbsp;word)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the probability of the specified word in the specified
 topic.</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_java.lang.Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class java.lang.<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</A></B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#clone()" title="class or interface in java.lang">clone</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#equals(java.lang.Object)" title="class or interface in java.lang">equals</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#finalize()" title="class or interface in java.lang">finalize</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#getClass()" title="class or interface in java.lang">getClass</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#hashCode()" title="class or interface in java.lang">hashCode</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#notify()" title="class or interface in java.lang">notify</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#notifyAll()" title="class or interface in java.lang">notifyAll</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#toString()" title="class or interface in java.lang">toString</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait()" title="class or interface in java.lang">wait</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait(long)" title="class or interface in java.lang">wait</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait(long, int)" title="class or interface in java.lang">wait</A></CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="LatentDirichletAllocation(double, double[][])"><!-- --></A><H3>
LatentDirichletAllocation</H3>
<PRE>
public <B>LatentDirichletAllocation</B>(double&nbsp;docTopicPrior,
                                 double[][]&nbsp;topicWordProbs)</PRE>
<DL>
<DD>Construct a latent Dirichelt allocation (LDA) model using the
 specified document-topic prior and topic-word distributions.

 <p>The topic-word probability array <code>topicWordProbs</code>
 represents a collection of discrete distributions
 <code>topicwordProbs[topic]</code> for topics, and thus must
 satisfy:

 <blockquote><pre>
 topicWordProbs[topic][word] &gt;= 0.0

 <big><big><big>&Sigma;</big></big></big><sub><sub>word &lt; numWords</sub></sub> topicWordProbs[topic][word] = 1.0</pre></blockquote>

 <p><b>Warning:</b> These requirements are <b>not</b> checked by the
 constructor.

 <p>See the class documentation above for an explanation of
 the parameters and what can be done with a model.
<P>
<DL>
<DT><B>Parameters:</B><DD><CODE>docTopicPrior</CODE> - The document-topic prior.<DD><CODE>topicWordProbs</CODE> - Array of discrete distributions over words,
 indexed by topic.
<DT><B>Throws:</B>
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IllegalArgumentException.html?is-external=true" title="class or interface in java.lang">IllegalArgumentException</A></CODE> - If the document-topic prior is
 not finite and positive, or if the topic-word probabilities
 arrays are not all the same length with entries between 0.0 and
 1.0 inclusive.</DL>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="numTopics()"><!-- --></A><H3>
numTopics</H3>
<PRE>
public int <B>numTopics</B>()</PRE>
<DL>
<DD>Returns the number of topics in this LDA model.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>The number of topics in this model.</DL>
</DD>
</DL>
<HR>

<A NAME="numWords()"><!-- --></A><H3>
numWords</H3>
<PRE>
public int <B>numWords</B>()</PRE>
<DL>
<DD>Returns the number of words on which this LDA model
 is based.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>The numbe of words in this model.</DL>
</DD>
</DL>
<HR>

<A NAME="documentTopicPrior()"><!-- --></A><H3>
documentTopicPrior</H3>
<PRE>
public double <B>documentTopicPrior</B>()</PRE>
<DL>
<DD>Returns the concentration value of the uniform Dirichlet prior over
 topic distributions for documents.  This value is effectively
 a prior count for topics used for additive smoothing during
 estimation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>The prior count of topics in documents.</DL>
</DD>
</DL>
<HR>

<A NAME="wordProbability(int, int)"><!-- --></A><H3>
wordProbability</H3>
<PRE>
public double <B>wordProbability</B>(int&nbsp;topic,
                              int&nbsp;word)</PRE>
<DL>
<DD>Returns the probability of the specified word in the specified
 topic.  The values returned should be non-negative and finite,
 and should sum to 1.0 over all words for a specifed topic.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>topic</CODE> - Topic identifier.<DD><CODE>word</CODE> - Word identifier.
<DT><B>Returns:</B><DD>Probability of the specified word in the specified
 topic.</DL>
</DD>
</DL>
<HR>

<A NAME="wordProbabilities(int)"><!-- --></A><H3>
wordProbabilities</H3>
<PRE>
public double[] <B>wordProbabilities</B>(int&nbsp;topic)</PRE>
<DL>
<DD>Returns an array representing of probabilities of words in the
 specified topic.  The probabilities are indexed by word
 identifier.

 <p>The returned result is a copy of the underlying data in
 the model so that changing it will not change the model.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>topic</CODE> - Topic identifier.
<DT><B>Returns:</B><DD>Array of probabilities of words in the specified topic.</DL>
</DD>
</DL>
<HR>

<A NAME="sampleTopics(int[], int, int, int, java.util.Random)"><!-- --></A><H3>
sampleTopics</H3>
<PRE>
public short[][] <B>sampleTopics</B>(int[]&nbsp;tokens,
                              int&nbsp;numSamples,
                              int&nbsp;burnin,
                              int&nbsp;sampleLag,
                              <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</PRE>
<DL>
<DD>Returns the specified number of Gibbs samples of topics for the
 specified tokens using the specified number of burnin epochs,
 the specified lag between samples, and the specified
 randomizer.  The array returned is an array of samples, each
 sample consisting of a topic assignment to each token in the
 specified list of tokens.  The tokens must all be in the appropriate
 range for this class

 <p>See the class documentation for more information on how the
 samples are computed.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tokens</CODE> - The tokens making up the document.<DD><CODE>numSamples</CODE> - Number of Gibbs samples to return.<DD><CODE>burnin</CODE> - The number of samples to take and throw away
 during the burnin period.<DD><CODE>sampleLag</CODE> - The interval between samples after burnin.<DD><CODE>random</CODE> - The random number generator to use for this sampling
 process.
<DT><B>Returns:</B><DD>The selection of topic samples generated by this
 sampler.
<DT><B>Throws:</B>
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IndexOutOfBoundsException.html?is-external=true" title="class or interface in java.lang">IndexOutOfBoundsException</A></CODE> - If there are tokens whose
 value is less than zero, or whose value is greater than the
 number of tokens in this model.
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IllegalArgumentException.html?is-external=true" title="class or interface in java.lang">IllegalArgumentException</A></CODE> - If the number of samples is
 not positive, the sample lag is not positive, or if the burnin
 period is negative.  if the number of samples, burnin, and lag
 are not positive numbers.</DL>
</DD>
</DL>
<HR>

<A NAME="bayesTopicEstimate(int[], int, int, int, java.util.Random)"><!-- --></A><H3>
bayesTopicEstimate</H3>
<PRE>
public double[] <B>bayesTopicEstimate</B>(int[]&nbsp;tokens,
                                   int&nbsp;numSamples,
                                   int&nbsp;burnin,
                                   int&nbsp;sampleLag,
                                   <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</PRE>
<DL>
<DD>Return the Bayesian point estimate of the topic distribution
 for a document consisting of the specified tokens, using Gibbs
 sampling with the specified parameters.  The Gibbs topic
 samples are simply averaged to produce the Bayesian estimate,
 which minimizes expected square loss.

 <p>See the method <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#sampleTopics(int[], int, int, int, java.util.Random)"><CODE>sampleTopics(int[],int,int,int,Random)</CODE></A>
 and the class documentation for more information on the sampling
 procedure.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>tokens</CODE> - The tokens making up the document.<DD><CODE>numSamples</CODE> - Number of Gibbs samples to return.<DD><CODE>burnin</CODE> - The number of samples to take and throw away
 during the burnin period.<DD><CODE>sampleLag</CODE> - The interval between samples after burnin.<DD><CODE>random</CODE> - The random number generator to use for this sampling
 process.
<DT><B>Returns:</B><DD>The selection of topic samples generated by this
 sampler.
<DT><B>Throws:</B>
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IndexOutOfBoundsException.html?is-external=true" title="class or interface in java.lang">IndexOutOfBoundsException</A></CODE> - If there are tokens whose
 value is less than zero, or whose value is greater than the
 number of tokens in this model.
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IllegalArgumentException.html?is-external=true" title="class or interface in java.lang">IllegalArgumentException</A></CODE> - If the number of samples is
 not positive, the sample lag is not positive, or if the burnin
 period is negative.</DL>
</DD>
</DL>
<HR>

<A NAME="gibbsSampler(int[][], short, double, double, int, int, int, java.util.Random, com.aliasi.corpus.ObjectHandler)"><!-- --></A><H3>
gibbsSampler</H3>
<PRE>
public static <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A> <B>gibbsSampler</B>(int[][]&nbsp;docWords,
                                                                 short&nbsp;numTopics,
                                                                 double&nbsp;docTopicPrior,
                                                                 double&nbsp;topicWordPrior,
                                                                 int&nbsp;burninEpochs,
                                                                 int&nbsp;sampleLag,
                                                                 int&nbsp;numSamples,
                                                                 <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random,
                                                                 <A HREF="../../../com/aliasi/corpus/ObjectHandler.html" title="interface in com.aliasi.corpus">ObjectHandler</A>&lt;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A>&gt;&nbsp;handler)</PRE>
<DL>
<DD>Run Gibbs sampling for the specified multinomial data, number
 of topics, priors, search parameters, randomization and
 callback sample handler.  Gibbs sampling provides samples from the
 posterior distribution of topic assignments given the corpus
 and prior hyperparameters.  A sample is encapsulated as an
 instance of class <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster"><CODE>LatentDirichletAllocation.GibbsSample</CODE></A>.  This method will return
 the final sample and also send intermediate samples to an
 optional handler.

 <p>The class documentation above explains Gibbs sampling for
 LDA as used in this method.

 <p>The primary input is an array of documents, where each
 document is represented as an array of integers representing
 the tokens that appear in it.  These tokens should be numbered
 contiguously from 0 for space efficiency.  The topic assignments
 in the Gibbs sample are aligned as parallel arrays to the array
 of documents.

 <p>The next three parameters are the hyperparameters of the
 model, specifically the number of topics, the prior count
 assigned to topics in a document, and the prior count assigned
 to words in topics.  A rule of thumb for the document-topic
 prior is to set it to 5 divided by the number of topics (or
 less if there are very few topics; 0.1 is typically the maximum
 value used).  A good general value for the topic-word prior is
 0.01.  Both of these priors will be diffuse and tend to lead to
 skewed posterior distributions.

 <p>The following three parameters specify how the sampling is
 to be done.  First, the sampler is &quot;burned in&quot; for a
 number of epochs specified by the burnin parameter.  After burn
 in, samples are taken after fixed numbers of documents to avoid
 correlation in the samples; the sampling frequency is specified
 by the sample lag.  Finally, the number of samples to be taken
 is specified.  For instance, if the burnin is 1000, the sample
 lag is 250, and the number of samples is 5, then samples are
 taken after 1000, 1250, 1500, 1750 and 2000 epochs.  If a
 non-null handler object is specified in the method call, its
 <code>handle(GibbsSample)</code> method is called with each the
 samples produced as above.

 <p>The final sample in the chain of samples is returned as the
 result.  Note that this sample will also have been passed to the
 specified handler as the last sample for the handler.

 <p>A random number generator must be supplied as an argument.
 This may just be a new instance of <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util"><CODE>Random</CODE></A> or
 a custom extension. It is used for all randomization in this
 method.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>docWords</CODE> - Corpus of documents to be processed.<DD><CODE>numTopics</CODE> - Number of latent topics to generate.<DD><CODE>docTopicPrior</CODE> - Prior count of topics in a document.<DD><CODE>topicWordPrior</CODE> - Prior count of words in a topic.<DD><CODE>burninEpochs</CODE> - Number of epochs to run before taking a sample.<DD><CODE>sampleLag</CODE> - Frequency between samples.<DD><CODE>numSamples</CODE> - Number of samples to take before exiting.<DD><CODE>random</CODE> - Random number generator.<DD><CODE>handler</CODE> - Handler to which the samples are sent.
<DT><B>Returns:</B><DD>The final Gibbs sample.</DL>
</DD>
</DL>
<HR>

<A NAME="gibbsSample(int[][], short, double, double, java.util.Random)"><!-- --></A><H3>
gibbsSample</H3>
<PRE>
public static <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Iterator.html?is-external=true" title="class or interface in java.util">Iterator</A>&lt;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster">LatentDirichletAllocation.GibbsSample</A>&gt; <B>gibbsSample</B>(int[][]&nbsp;docWords,
                                                                          short&nbsp;numTopics,
                                                                          double&nbsp;docTopicPrior,
                                                                          double&nbsp;topicWordPrior,
                                                                          <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/util/Random.html?is-external=true" title="class or interface in java.util">Random</A>&nbsp;random)</PRE>
<DL>
<DD>Return an iterator over Gibbs samples for the specified
 document-word corpus, number of topics, priors and randomizer.
 These are the same Gibbs samples as wold be produced by the
 method <A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.html#gibbsSampler(int[][], short, double, double, int, int, int, java.util.Random, com.aliasi.corpus.ObjectHandler)"><CODE>gibbsSampler(int[][],short,double,double,int,int,int,Random,ObjectHandler)</CODE></A>.
 See that method and the class documentation for more details.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>docWords</CODE> - Corpus of documents to be processed.<DD><CODE>numTopics</CODE> - Number of latent topics to generate.<DD><CODE>docTopicPrior</CODE> - Prior count of topics in a document.<DD><CODE>topicWordPrior</CODE> - Prior count of words in a topic.<DD><CODE>random</CODE> - Random number generator.</DL>
</DD>
</DL>
<HR>

<A NAME="tokenizeDocuments(java.lang.CharSequence[], com.aliasi.tokenizer.TokenizerFactory, com.aliasi.symbol.SymbolTable, int)"><!-- --></A><H3>
tokenizeDocuments</H3>
<PRE>
public static int[][] <B>tokenizeDocuments</B>(<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/CharSequence.html?is-external=true" title="class or interface in java.lang">CharSequence</A>[]&nbsp;texts,
                                        <A HREF="../../../com/aliasi/tokenizer/TokenizerFactory.html" title="interface in com.aliasi.tokenizer">TokenizerFactory</A>&nbsp;tokenizerFactory,
                                        <A HREF="../../../com/aliasi/symbol/SymbolTable.html" title="interface in com.aliasi.symbol">SymbolTable</A>&nbsp;symbolTable,
                                        int&nbsp;minCount)</PRE>
<DL>
<DD>Tokenize an array of text documents represented as character
 sequences into a form usable by LDA, using the specified
 tokenizer factory and symbol table.  The symbol table should be
 constructed fresh for this application, but may be used after
 this method is called for further token to symbol conversions.
 Only tokens whose count is equal to or larger the specified
 minimum count are included.  Only tokens whose count exceeds
 the minimum are added to the symbol table, thus producing a
 compact set of symbol assignments to tokens for downstream
 processing.

 <p><i>Warning</i>: With some tokenizer factories and or minimum
 count thresholds, there may be documents with no tokens in
 them.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>texts</CODE> - The text corpus.<DD><CODE>tokenizerFactory</CODE> - A tokenizer factory for tokenizing the texts.<DD><CODE>symbolTable</CODE> - Symbol table used to convert tokens to identifiers.<DD><CODE>minCount</CODE> - Minimum count for a token to be included in a
 document's representation.
<DT><B>Returns:</B><DD>The tokenized form of a document suitable for input to LDA.</DL>
</DD>
</DL>
<HR>

<A NAME="tokenizeDocument(java.lang.CharSequence, com.aliasi.tokenizer.TokenizerFactory, com.aliasi.symbol.SymbolTable)"><!-- --></A><H3>
tokenizeDocument</H3>
<PRE>
public static int[] <B>tokenizeDocument</B>(<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/CharSequence.html?is-external=true" title="class or interface in java.lang">CharSequence</A>&nbsp;text,
                                     <A HREF="../../../com/aliasi/tokenizer/TokenizerFactory.html" title="interface in com.aliasi.tokenizer">TokenizerFactory</A>&nbsp;tokenizerFactory,
                                     <A HREF="../../../com/aliasi/symbol/SymbolTable.html" title="interface in com.aliasi.symbol">SymbolTable</A>&nbsp;symbolTable)</PRE>
<DL>
<DD>Tokenizes the specified text document using the specified tokenizer
 factory returning only tokens that exist in the symbol table.  This
 method is useful within a given LDA model for tokenizing new documents
 into lists of words.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>text</CODE> - Character sequence to tokenize.<DD><CODE>tokenizerFactory</CODE> - Tokenizer factory for tokenization.<DD><CODE>symbolTable</CODE> - Symbol table to use for converting tokens
 to symbols.
<DT><B>Returns:</B><DD>The array of integer symbols for tokens that exist in
 the symbol table.</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../com/aliasi/cluster/KMeansClusterer.html" title="class in com.aliasi.cluster"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../com/aliasi/cluster/LatentDirichletAllocation.GibbsSample.html" title="class in com.aliasi.cluster"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../index.html?com/aliasi/cluster/LatentDirichletAllocation.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="LatentDirichletAllocation.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;<A HREF="#nested_class_summary">NESTED</A>&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
